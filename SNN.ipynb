{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "505fa1e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torchvision.datasets\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1eb22ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_set_loader, optimizer, epoch, logging_interval=100):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_set_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad() \n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch_idx % logging_interval == 0:\n",
    "            pred = output.max(1, keepdim=True)[1]\n",
    "            correct = pred.eq(target.view_as(pred)).float().mean().item()\n",
    "            \n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)] Loss: {:.6f} Accuracy: {:.2f}%'.format(\n",
    "                epoch, batch_idx * len(data), len(train_set_loader.dataset),\n",
    "                100. * batch_idx / len(train_set_loader), loss.item(),\n",
    "                100. * correct))\n",
    "            \n",
    "def train_many_epochs(model):\n",
    "    epoch = 1\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.5)\n",
    "    train(model, device, train_set_loader, optimizer, epoch, logging_interval=10)\n",
    "    test(model, device, test_set_loader)\n",
    "    \n",
    "    epoch = 2\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.05, momentum=0.5)\n",
    "    train(model, device, train_set_loader, optimizer, epoch, logging_interval=10)\n",
    "    test(model, device, test_set_loader)\n",
    "\n",
    "    epoch = 3\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.5)\n",
    "    train(model, device, train_set_loader, optimizer, epoch, logging_interval=10)\n",
    "    test(model, device, test_set_loader)\n",
    "    \n",
    "    \n",
    "def test(model, device, test_set_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, target in test_set_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            \n",
    "            test_loss += F.nll_loss(output, target, reduce=True).item()\n",
    "            pred = output.max(1, keepdim=True)[1]\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "            \n",
    "    test_loss /= len(test_set_loader.dataset)\n",
    "    print()\n",
    "    print('Test set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)'.format(\n",
    "        test_loss,\n",
    "        correct, len(test_set_loader.dataset),\n",
    "        100. * correct / len(test_set_loader.dataset)))\n",
    "    print(\"\")\n",
    "    \n",
    "def download_mnist(data_path):\n",
    "    if not os.path.exists(data_path):\n",
    "        os.mkdir(data_path)\n",
    "    transformation = transforms.Compose(\n",
    "        [transforms.ToTensor(), transforms.Normalize((0.5,), (1.0,))])\n",
    "    training_set = torchvision.datasets.MNIST(\n",
    "        data_path, train=True, transform=transformation, download=True)\n",
    "    testing_set = torchvision.datasets.MNIST(\n",
    "        data_path, train=False, transform=transformation, download=True)\n",
    "    return training_set, testing_set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e2d4ea7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1000\n",
    "DATA_PATH = './data'\n",
    "\n",
    "training_set, testing_set = download_mnist(DATA_PATH)\n",
    "train_set_loader = torch.utils.data.DataLoader(\n",
    "    dataset=training_set,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True)\n",
    "\n",
    "test_set_loader = torch.utils.data.DataLoader(\n",
    "    dataset=testing_set,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ee9f64be",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpikingNeuronLayerRNN(nn.Module):\n",
    "    def __init__(self, device, n_inputs=28*28, n_hidden=100,\n",
    "                decay_multiplier=0.9, threshold=2.0, penalty_threshold=2.5):\n",
    "        super(SpikingNeuronLayerRNN, self).__init__()\n",
    "        \n",
    "        self.device = device\n",
    "        self.n_inputs = n_inputs\n",
    "        self.n_hidden = n_hidden\n",
    "        self.decay_multiplier = decay_multiplier\n",
    "        self.threshold = threshold\n",
    "        self.penalty_threshold = penalty_threshold\n",
    "        \n",
    "        self.fc = nn.Linear(n_inputs, n_hidden)\n",
    "        \n",
    "        self.init_parameters() \n",
    "        self.reset_state()\n",
    "        self.to(self.device)\n",
    "    \n",
    "    def init_parameters(self):\n",
    "        for param in self.parameters():\n",
    "            if param.dim() >= 2:\n",
    "                nn.init.xavier_uniform_(param)\n",
    "    \n",
    "    def reset_state(self):\n",
    "        self.prev_inner = torch.zeros([self.n_hidden]).to(self.device)\n",
    "        self.prev_outer = torch.zeros([self.n_hidden]).to(self.device)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        if self.prev_inner.dim() == 1:\n",
    "            batch_size = x.shape[0]\n",
    "            self.prev_inner = torch.stack(batch_size * [self.prev_inner])\n",
    "            self.prev_outer = torch.stack(batch_size * [self.prev_outer])\n",
    "            \n",
    "        input_excitation = self.fc(x)\n",
    "        \n",
    "        inner_excitation = input_excitation + self.prev_inner * self.decay_multiplier\n",
    "        outer_excitation = F.relu(inner_excitation - self.threshold)\n",
    "        \n",
    "        do_penalize_gate = (outer_excitation > 0).float()\n",
    "        inner_excitation = inner_excitation - do_penalize_gate * (self.penalty_threshold/self.threshold * inner_excitation)\n",
    "        \n",
    "        delayed_return_state = self.prev_inner \n",
    "        delayed_return_output = self.prev_outer\n",
    "        self.prev_inner = inner_excitation\n",
    "        self.prev_outer = outer_excitation\n",
    "        return delayed_return_state, delayed_return_output\n",
    "    \n",
    "class InputDataToSpikingPerceptronLayer(nn.Module):\n",
    "    def __init__(self, device):\n",
    "        super(InputDataToSpikingPerceptronLayer, self).__init__()\n",
    "        self.device = device \n",
    "        self.reset_state()\n",
    "        self.to(self.device)\n",
    "        \n",
    "    def reset_state(self):\n",
    "        pass\n",
    "    \n",
    "    def forward(self, x, is_2D=True):\n",
    "        if is_2D:\n",
    "            x = x.view(x.size(0), -1)\n",
    "        random_activation_perceptron = torch.rand(x.shape).to(self.device)\n",
    "        return random_activation_perceptron * x\n",
    "    \n",
    "class OutputDataToSpikingPerceptronLayer(nn.Module):\n",
    "    def __init__(self, average_output=True):\n",
    "        super(OutputDataToSpikingPerceptronLayer, self).__init__()\n",
    "        if average_output:\n",
    "            self.reducer = lambda x, dim: x.sum(dim=dim)\n",
    "        else:\n",
    "            self.reducer = lambda x, dim: x.mean(dim=dim)\n",
    "            \n",
    "    def forward(self, x):\n",
    "        if type(x) == list:\n",
    "            x = torch.stack(x)\n",
    "        return self.reducer(x, 0)\n",
    "    \n",
    "class SpikingNet(nn.Module):\n",
    "    def __init__(self, device, n_time_steps, begin_eval):\n",
    "        super(SpikingNet, self).__init__()\n",
    "        assert( 0 <= begin_eval and begin_eval < n_time_steps)\n",
    "        self.deice = device \n",
    "        self.n_time_steps = n_time_steps\n",
    "        self.begin_eval = begin_eval\n",
    "        \n",
    "        self.input_conversion = InputDataToSpikingPerceptronLayer(device)\n",
    "        \n",
    "        self.layer1 = SpikingNeuronLayerRNN(\n",
    "            device, n_inputs=28*28, n_hidden=100, decay_multiplier=0.9, threshold=1.0, penalty_threshold=1.5)\n",
    "        \n",
    "        self.layer2 = SpikingNeuronLayerRNN(\n",
    "            device, n_inputs=100, n_hidden=10, decay_multiplier=0.9, threshold=1.0, penalty_threshold=1.5)\n",
    "        \n",
    "        self.output_conversion = OutputDataToSpikingPerceptronLayer(average_output=False)\n",
    "        \n",
    "        self.to(device)\n",
    "        \n",
    "    def forward_through_time(self, x):\n",
    "        self.input_conversion.reset_state()\n",
    "        self.layer1.reset_state()\n",
    "        self.layer2.reset_state()\n",
    "        \n",
    "        out = []\n",
    "        \n",
    "        all_layer1_states = []\n",
    "        all_layer1_outputs = []\n",
    "        \n",
    "        all_layer2_states = []\n",
    "        all_layer2_outputs = []\n",
    "        \n",
    "        for _ in range(self.n_time_steps):\n",
    "            xi = self.input_conversion(x)\n",
    "            \n",
    "            layer1_state, layer1_output = self.layer1(xi)\n",
    "            layer2_state, layer2_output = self.layer2(layer1_output)\n",
    "            \n",
    "            all_layer1_states.append(layer1_state)\n",
    "            all_layer1_outputs.append(layer1_output)\n",
    "            \n",
    "            all_layer2_states.append(layer2_state)\n",
    "            all_layer2_outputs.append(layer2_output)\n",
    "            out.append(layer2_state)\n",
    "        \n",
    "        out = self.output_conversion(out[self.begin_eval:])\n",
    "        return out, [[all_layer1_states, all_layer1_outputs], \n",
    "                     [all_layer2_states, all_layer2_outputs]]\n",
    " \n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.forward_through_time(x)\n",
    "        return F.log_softmax(out, dim=-1)\n",
    "    \n",
    "    \n",
    "    def visualize_all_neurons(self, x):\n",
    "\n",
    "        assert x.shape[0] == 1 and len(x.shape) == 4, (\n",
    "\n",
    "            \"Pass only 1 example to SpikingNet.visualize(x) with outer dimension shape of 1.\")\n",
    "        _, layers_state = self.forward_through_time(x)\n",
    " \n",
    "        for i, (all_layer_states, all_layer_outputs) in enumerate(layers_state):\n",
    "            layer_state  =  torch.stack(all_layer_states).data.cpu(\n",
    "                ).numpy().squeeze().transpose()\n",
    "            layer_output = torch.stack(all_layer_outputs).data.cpu(\n",
    "                ).numpy().squeeze().transpose()\n",
    " \n",
    "\n",
    "            self.plot_layer(layer_state, title=\"Inner state values of neurons for layer {}\".format(i))\n",
    "            self.plot_layer(layer_output, title=\"Output spikes (activation) values of neurons for layer {}\".format(i))\n",
    "\n",
    "    def visualize_neuron(self, x, layer_idx, neuron_idx):\n",
    "        assert x.shape[0] == 1 and len(x.shape) == 4, (\n",
    "            \"Pass only 1 example to SpikingNet.visualize(x) with outer dimension shape of 1.\")\n",
    "        _, layers_state = self.forward_through_time(x)\n",
    "\n",
    "        all_layer_states, all_layer_outputs = layers_state[layer_idx]\n",
    "        layer_state  =  torch.stack(all_layer_states).data.cpu(\n",
    "            ).numpy().squeeze().transpose()\n",
    "        layer_output = torch.stack(all_layer_outputs).data.cpu(\n",
    "            ).numpy().squeeze().transpose()\n",
    "\n",
    "        self.plot_neuron(\n",
    "            layer_state[neuron_idx], \n",
    "            title=\"Inner state values neuron {} of layer {}\".format(neuron_idx, layer_idx))\n",
    "        self.plot_neuron(\n",
    "            layer_output[neuron_idx], \n",
    "            title=\"Output spikes (activation) values of neuron {} of layer {}\".format(neuron_idx, layer_idx))\n",
    " \n",
    "    def plot_layer(self, layer_values, title):\n",
    "        \"\"\"\n",
    "        This function is derived from:\n",
    "            https://github.com/guillaume-chevalier/LSTM-Human-Activity-Recognition\n",
    "        Which was released under the MIT License.\n",
    "        \"\"\"\n",
    "        width = max(16, layer_values.shape[0] / 8)\n",
    "        height = max(4, layer_values.shape[1] / 8)\n",
    "        plt.figure(figsize=(width, height))\n",
    "        plt.imshow(\n",
    "            layer_values,\n",
    "            interpolation=\"nearest\",\n",
    "            cmap=plt.cm.rainbow\n",
    "        )\n",
    "        \n",
    "        plt.title(title)\n",
    "        plt.colorbar()\n",
    "        plt.xlabel(\"Time\")\n",
    "        plt.ylabel(\"Neurons of layer\")\n",
    "        plt.show()\n",
    " \n",
    "    def plot_neuron(self, neuron_through_time, title):\n",
    "        width = max(16, len(neuron_through_time) / 8)\n",
    "        height = 4\n",
    "        plt.figure(figsize=(width, height))\n",
    "        plt.title(title)\n",
    "        plt.plot(neuron_through_time)\n",
    "        plt.xlabel(\"Time\")\n",
    "        plt.ylabel(\"Neuron's activation\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd8729be",
   "metadata": {},
   "source": [
    "# Training SNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "369b362d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)] Loss: 2.368621 Accuracy: 11.30%\n",
      "Train Epoch: 1 [10000/60000 (17%)] Loss: 1.999440 Accuracy: 25.50%\n",
      "Train Epoch: 1 [20000/60000 (33%)] Loss: 0.921388 Accuracy: 66.00%\n",
      "Train Epoch: 1 [30000/60000 (50%)] Loss: 0.476034 Accuracy: 87.10%\n",
      "Train Epoch: 1 [40000/60000 (67%)] Loss: 0.438016 Accuracy: 87.20%\n",
      "Train Epoch: 1 [50000/60000 (83%)] Loss: 0.463105 Accuracy: 85.90%\n",
      "\n",
      "Test set: Average loss: 0.0005, Accuracy: 8575/10000 (85.75%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)] Loss: 0.517292 Accuracy: 83.20%\n",
      "Train Epoch: 2 [10000/60000 (17%)] Loss: 0.366081 Accuracy: 88.60%\n",
      "Train Epoch: 2 [20000/60000 (33%)] Loss: 0.362762 Accuracy: 89.30%\n",
      "Train Epoch: 2 [30000/60000 (50%)] Loss: 0.311561 Accuracy: 91.20%\n",
      "Train Epoch: 2 [40000/60000 (67%)] Loss: 0.357040 Accuracy: 89.70%\n",
      "Train Epoch: 2 [50000/60000 (83%)] Loss: 0.349266 Accuracy: 89.20%\n",
      "\n",
      "Test set: Average loss: 0.0003, Accuracy: 9027/10000 (90.27%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)] Loss: 0.319462 Accuracy: 90.40%\n",
      "Train Epoch: 3 [10000/60000 (17%)] Loss: 0.313489 Accuracy: 90.80%\n",
      "Train Epoch: 3 [20000/60000 (33%)] Loss: 0.346861 Accuracy: 90.10%\n",
      "Train Epoch: 3 [30000/60000 (50%)] Loss: 0.380699 Accuracy: 88.70%\n",
      "Train Epoch: 3 [40000/60000 (67%)] Loss: 0.349198 Accuracy: 89.10%\n",
      "Train Epoch: 3 [50000/60000 (83%)] Loss: 0.339999 Accuracy: 91.00%\n",
      "\n",
      "Test set: Average loss: 0.0003, Accuracy: 9088/10000 (90.88%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spiking_model = SpikingNet(device, n_time_steps = 128, begin_eval=0)\n",
    "train_many_epochs(spiking_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69d7480",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
